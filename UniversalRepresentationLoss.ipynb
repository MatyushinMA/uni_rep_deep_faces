{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import numpy.random as npr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalRepresentationLoss(object):\n",
    "    def __init__(self,\n",
    "                 emb_size, # size of embedding\n",
    "                 margin=30, # m parameter for idt loss\n",
    "                 num_groups=16, # K parameter - num of sub-embeddings\n",
    "                 num_variations=3, # M parameter for discriminator\n",
    "                 masks=None, # masks V_t of shape (M, K)\n",
    "                 discriminator=None, # discriminator model (BS, emb_size) -> (BS, M)\n",
    "                 discriminator_lr=1e-5, # learning rate to fit discriminator\n",
    "                 l_reg=0.1, # regularization coefficient\n",
    "                 l_cls=0.1, # classification coefficient\n",
    "                 l_adv=0.1 # adversarial coefficient\n",
    "                ):\n",
    "        self.emb_size = emb_size\n",
    "        self.margin = margin\n",
    "        self.num_groups = num_groups\n",
    "        assert self.emb_size % self.num_groups == 0\n",
    "        self.sub_emb_size = self.emb_size//self.num_groups\n",
    "        self.num_variations = num_variations\n",
    "\n",
    "        # gen masks\n",
    "        if masks is not None:\n",
    "            self.masks = masks\n",
    "            assert np.all(np.array(self.masks.shape) == np.array([self.num_variations, self.num_groups]))\n",
    "            assert self._check_masks()\n",
    "        else:\n",
    "            self.masks = []\n",
    "            while len(self.masks) < self.num_variations:\n",
    "                while True:\n",
    "                    new_idx = npr.choice(self.num_groups, size=self.num_groups//2, replace=False)\n",
    "                    new_mask = np.zeros(self.num_groups, dtype=bool)\n",
    "                    new_mask[new_idx] = 1\n",
    "                    self.masks.append(new_mask)\n",
    "                    if self._check_masks():\n",
    "                        break\n",
    "                    else:\n",
    "                        del self.masks[-1]\n",
    "            self.masks = np.stack(self.masks)\n",
    "\n",
    "        # build discriminator\n",
    "        if discriminator is not None:\n",
    "            self.discriminator = discriminator\n",
    "        else:\n",
    "            class LinearDiscriminator(nn.Module):\n",
    "                def __init__(self):\n",
    "                    super().__init__()\n",
    "\n",
    "                    self.layer = nn.Linear(emb_size, num_variations)\n",
    "                    self.act = nn.Sigmoid()\n",
    "\n",
    "                def forward(self, x):\n",
    "                    return self.act(self.layer(x))\n",
    "            self.discriminator = LinearDiscriminator()\n",
    "        self.discriminator_optimizer = torch.optim.Adam(self.discriminator.parameters(),\n",
    "                                                        lr=discriminator_lr)\n",
    "        \n",
    "\n",
    "        # save coefficients\n",
    "        self.l_reg = l_reg\n",
    "        self.l_cls = l_cls\n",
    "        self.l_adv = l_adv\n",
    "    \n",
    "    def _check_masks(self):\n",
    "        for i in range(len(self.masks)):\n",
    "            for j in range(i + 1, len(self.masks)):\n",
    "                i_mask = self.masks[i]\n",
    "                j_mask = self.masks[j]\n",
    "                if np.all(i_mask == j_mask):\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    def _freeze_discriminator(self):\n",
    "        for p in self.discriminator.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.discriminator.eval()\n",
    "    \n",
    "    def _unfreeze_discriminator(self):\n",
    "        for p in self.discriminator.parameters():\n",
    "            p.requires_grad = True\n",
    "        self.discriminator.train()\n",
    "    \n",
    "    def __call__(self,\n",
    "                 features, # (BS, emb_size)\n",
    "                 conf, # (BS, num_groups)\n",
    "                 prototypes, # (num_classes, emb_size)\n",
    "                 target, # (BS, 1)\n",
    "                 var_target # (BS, num_variations)\n",
    "                ):\n",
    "        # idt loss\n",
    "        grouped_features = features.reshape((-1, self.num_groups, self.sub_emb_size))\n",
    "        normed_grouped_features = grouped_features/torch.norm(grouped_features, dim=-1, keepdim=True)\n",
    "        grouped_prototypes = prototypes.reshape((-1, self.num_groups, self.sub_emb_size))\n",
    "        \n",
    "        target_prototypes = grouped_prototypes[target.flatten()]\n",
    "        target_dot_features = torch.einsum('ijk,ijk->ij', grouped_features, target_prototypes)\n",
    "        conf_weighted_target_dot_features = conf*target_dot_features\n",
    "        target_margins = conf_weighted_target_dot_features.sum(dim=-1)/self.num_groups\n",
    "        \n",
    "        extra_margins = torch.empty()\n",
    "        for i, (f_i, s_i) in enumerate(zip(grouped_features, conf)):\n",
    "            for j, w_j in enumerate(grouped_prototypes):\n",
    "                if target[i] == j:\n",
    "                    continue\n",
    "                extra_dot_features = torch.einsum('jk,jk->j', f_i, w_j)\n",
    "                conf_weighted_extra_dot_features = extra_dot_features*s_i\n",
    "                extra_margins += torch.exp(conf_weighted_extra_dot_features.sum(dim=-1)/self.num_groups)\n",
    "        \n",
    "        exp_margins = torch.exp(target_margins - self.margin)\n",
    "        loss_idt = -torch.log(exp_margins/(exp_margins + extra_margins))\n",
    "        \n",
    "        # reg loss\n",
    "        loss_reg = (conf**2).sum(dim=-1)/self.num_groups\n",
    "        \n",
    "        # cls and adv loss\n",
    "        self._freeze_discriminator()\n",
    "        masked_features = (features.unsqueeze(0)*self.masks.unsqueeze(1))\n",
    "        masked_flat_features = masked_features.reshape((self.num_variations, -1, self.emb_size))\n",
    "        discriminator_predict = self.discriminator(masked_flat_features)\n",
    "        self.x = masked_flat_features.clone()\n",
    "        self.y = var_target.clone()\n",
    "        loss_cls = torch.empty()\n",
    "        loss_adv = torch.empty()\n",
    "        for t in range(self.num_variations):\n",
    "            t_discriminator_predict = discriminator_predict[t]\n",
    "            p_discriminator = t_discriminator_predict[:, t]*var_target[:, t] + \\\n",
    "                              (1 - t_discriminator_predict[:, t])*(1 - var_target[:, t])\n",
    "            loss_cls += -torch.log(p_discriminator)\n",
    "            for _t in range(self.num_variations):\n",
    "                if _t == t:\n",
    "                    continue\n",
    "                p_discriminator_0 = 1 - t_discriminator_predict[:, _t]\n",
    "                p_discriminator_1 = t_discriminator_predict[:, _t]\n",
    "                loss_adv += -0.5*(torch.log(p_discriminator_0) + torch.log(p_discriminator_1))\n",
    "        \n",
    "        loss = loss_idt + \\\n",
    "               self.l_reg*loss_reg + \\\n",
    "               self.l_cls*loss_cls + \\\n",
    "               self.l_adv*loss_adv\n",
    "        return loss.mean(dim=0)\n",
    "    \n",
    "    def update_discriminator(self, it=1):\n",
    "        for _ in range(it):\n",
    "            self._unfreeze_deiscriminator()\n",
    "            predict = self.discriminator(self.x)\n",
    "            loss = torch.empty()\n",
    "            for t in range(self.num_variations):\n",
    "                target_proba = predict[t]*self.y + (1 - predict[t])*(1 - self.y)\n",
    "                log_target_proba = torch.log(target_proba)\n",
    "                loss += -log_target_proba.sum(dim=-1)\n",
    "            loss = loss.mean(dim=0)\n",
    "            loss.backward()\n",
    "            self.discriminator_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
